{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "configured-episode",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "from string import Template\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "postal-greek",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datadir = \"../data/experiments\"\n",
    "out_path = \"../notebooks/latex_array.txt\"\n",
    "filepath = \"f1_{exp}_{dataset}.json\"\n",
    "datasets = [\"doccano1\", \"hate_speech\", \"unhealthy\"]\n",
    "dataset_label_names = [\"doccano\", \"mhs\", \"uc\"]\n",
    "dataset_names = [\"Doccano 1.0\", \"MHS\", \"UC\"]\n",
    "experiments = [\"AA\", \"BA\", \"increment\", \"single_training\", \"threshold\"]\n",
    "experiment_names = [\"Self-supervised\", \"\", \"Incremental\", \"Single-label vs. Multi-label\", \"Threshold\"]\n",
    "LABEL_COLUMNS = {\n",
    "    \"doccano1\": [\"Positive\", \"Negative\", \"Joy\", \"Delight\", \"Inspiration\", \"Calm\", \"Surprise\", \"Compassion\", \"Fear\", \"Sadness\", \"Repulsion\", \"Anger\", \"Ironic\", \"Embarrassing\", \"Vulgar\", \"Political\", \"Interesting\", \"Understandable\", \"Incomprehensible\", \"Offensive to me\", \"Offensive to someone\", \"Funny to me\", \"Funny to someone\"],\n",
    "    \"hate_speech\": [\"Sentiment\",\"Respect\",\"Insult\",\"Humiliate\",\"Status\",\"Dehumanize\",\"Violence\",\"Genocide\",\"Attack Defend\",\"Hate Speech\"],\n",
    "    \"unhealthy\": [\"Antagonize\",\"Condescending\",\"Dismissive\",\"Generalisation\",\"Hostile\",\"Sarcastic\"],\n",
    "}\n",
    "iterators = {\n",
    "    \"threshold\": (0.1, 0.15, 0.2, 0.25),\n",
    "    \"increment\": list(range(1,9)),\n",
    "    \"single_training\": LABEL_COLUMNS,\n",
    "    \"AA\": None,\n",
    "    \"BA\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "signal-threshold",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# table_template = \"\"\"\n",
    "# \\begin{table*}%[H]\n",
    "# \\centering\n",
    "# \\begin{adjustbox}{width=\\textwidth}\n",
    "# \\begin{tabular}{l|ccc}\n",
    "# \\hline\n",
    "#  & F1 macro & F1 score (class 1) & F1 score (class 0) & F1 score (mean for 1 and 2 class) & & AER & AAL & MLARL\\\\\n",
    "# \\hline\n",
    "# \\multirow{2}{*}{Number of labels} & 23, i.e. delight, offensive to me, & 8, i.e. hostile, sarcastic,  & 10, i.e. violence,  \\\\\n",
    "\n",
    "# \\hline\n",
    "# \\end{tabular}\n",
    "# \\end{adjustbox}\n",
    "# \\caption{Dataset profiles after pre-processing. Each dataset contains a set number of labels, which are explained in full detail in Section \\ref{sec:datasets}.}\n",
    "# \\label{tab:datasets}\n",
    "# \\end{table*}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "dataset_template = Template(\"\"\"\n",
    "\\\\begin{table}[H]\n",
    "\\\\centering\n",
    "\\\\begin{adjustbox}{width=\\\\textwidth}\n",
    "\\\\begin{tabular}{l|l|$columns_c_string}\n",
    "\\\\hline\n",
    "Metric & $columns_table\\\\\\\\\n",
    "\\\\hline\n",
    "\\\\multirow{$variant_num}{*}{F1 macro} $f1_macro_scores\n",
    "\\\\hline\n",
    "\\\\multirow{$variant_num}{*}{F1 score (class 0)} $f1_class0_scores \n",
    "\\\\hline\n",
    "\\\\multirow{$variant_num}{*}{F1 score (class 1)} $f1_class1_scores \n",
    "\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\end{adjustbox}\n",
    "\\\\caption{$experiment_name results for dataset $dataset_name}\n",
    "\\\\label{tab:label_metrics_$dataset_label_name}\n",
    "\\\\end{table}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "crazy-verse",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(join(datadir, \"test.txt\"), \"w\") as f:\n",
    "#     str_ = dataset_template.substitute(\n",
    "#         columns_c_string=\"c\"*2,\n",
    "#         columns_table=\"cos & cos2\",\n",
    "#         f1_macro_scores=\"1&2\",\n",
    "#         f1_class0_scores=\"3&4\",\n",
    "#         f1_class1_scores=\"5&6\",\n",
    "#         dataset_name=\"Doccano 1.0\",\n",
    "#         dataset_label_name=\"doccano\"\n",
    "#     )\n",
    "#     f.write(str_)\n",
    "\n",
    "\n",
    "def get_mean_and_std(data, metric):\n",
    "    processed = []\n",
    "    for k, v in data.items():\n",
    "        processed.append(v[metric])\n",
    "    processed = np.stack(processed)\n",
    "    # print(processed)\n",
    "    return processed.mean(axis=0), processed.std(axis=0)\n",
    "\n",
    "\n",
    "metrics = (\"macro_per_dim\", \"0_per_dim\", \"1_per_dim\")\n",
    "def generate_single_table(dataset_idx, experiment_idx, experiment_column=\"$t$ value\", experiment_template=\"th_{v}\", experiment_iterator=(0.1, 0.15, 0.2, 0.25)):\n",
    "    dataset = datasets[dataset_idx]\n",
    "    datafile = join(datadir, filepath.format(dataset=dataset, exp=experiments[experiment_idx]))\n",
    "    with open(datafile, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "#     print(data)\n",
    "#     means, stds = get_mean_and_std(data[\"th_0.1\"], \"macro_per_dim\")\n",
    "    column_c_string = \"c\"*(len(LABEL_COLUMNS[dataset]))\n",
    "    columns_table = experiment_column + \"&\" + \" & \".join(LABEL_COLUMNS[dataset])\n",
    "    metric_rows = {}\n",
    "    for metric in metrics:\n",
    "        metric_one_row = \"\"\n",
    "        for i, key in enumerate(experiment_iterator):\n",
    "            means, stds = get_mean_and_std(data[experiment_template.format(v=key)], metric)\n",
    "            curr_data = zip(list(means), list(stds))\n",
    "            curr_row = \"& \" + str(key) + \" & \" + \" & \".join(map(lambda d: \"$\" + f\"{d[0]:.2f}\"+\"\\\\pm\"+f\"{d[1]:.2f}\" + \"$\", curr_data))\n",
    "            metric_one_row += curr_row\n",
    "            metric_one_row += \"\\\\\\\\\\n\"\n",
    "        metric_rows[metric] = metric_one_row\n",
    "    \n",
    "    output = dataset_template.substitute(\n",
    "        columns_c_string=column_c_string,\n",
    "        columns_table=columns_table,\n",
    "        variant_num=i+1,\n",
    "        f1_macro_scores=metric_rows[metrics[0]],\n",
    "        f1_class0_scores=metric_rows[metrics[1]],\n",
    "        f1_class1_scores=metric_rows[metrics[2]],\n",
    "        dataset_name=dataset_names[dataset_idx],\n",
    "        dataset_label_name=dataset_label_names[dataset_idx],\n",
    "        experiment_name=experiment_names[experiment_idx]\n",
    "    )\n",
    "    return  output\n",
    "\n",
    "def generate_single_training_table(dataset_idx):\n",
    "    experiment_column = \"Classification type\"\n",
    "    dataset = datasets[dataset_idx]\n",
    "    column_c_string = \"c\"*(len(LABEL_COLUMNS[dataset])+1)\n",
    "    columns_table = experiment_column + \"&\" + \" &\".join(LABEL_COLUMNS[dataset]) + \"& All Labels (average)\"\n",
    "    datafile = join(datadir, filepath.format(dataset=dataset, exp=\"single_training\"))\n",
    "    with open(datafile, \"r\") as f:\n",
    "        single_data = json.load(f)\n",
    "    with open(join(datadir, filepath.format(dataset=dataset, exp=\"threshold\")), \"r\") as f:\n",
    "        multi_data = json.load(f)[\"th_0.15\"]\n",
    "    \n",
    "    metric_rows = []\n",
    "    for metric_single, metric_multi in zip((\"macro\", \"mean_for_0\", \"mean_for_1\"), metrics):\n",
    "        metric_single_row = \"&Single-label\"\n",
    "        metric_multi_row = \"&Multi-label\"\n",
    "        means, stds = get_mean_and_std(multi_data, metric_multi)\n",
    "        multi_data_in_row = list(zip(list(means), list(stds)))\n",
    "        multi_average = (means.mean(), means.std())\n",
    "        single_average=[]\n",
    "        for i, label in enumerate(single_data.keys()):\n",
    "            curr_data = get_mean_and_std(single_data[label], metric_single)\n",
    "            single_average.append(curr_data[0])\n",
    "            metric_single_row += f\"&${curr_data[0]:.2f}\\\\pm{curr_data[1]:.2f}$\"\n",
    "            curr_multi_data = multi_data_in_row[i]\n",
    "            metric_multi_row += f\"&${curr_multi_data[0]:.2f}\\\\pm{curr_multi_data[1]:.2f}$\"\n",
    "            \n",
    "            # curr_row = \"& \" + str(key) + \" & \" + \" & \".join(map(lambda d: \"$\" + f\"{d[0]:.2f}\"+\"\\\\pm\"+f\"{d[1]:.2f}\" + \"$\", curr_data))\n",
    "            # curr_row += \"\\\\\\\\\\n\"\n",
    "            # metric_one_row += curr_row\n",
    "            # mean, std = get_mean_and_std(multi_data[label], metric_single)\n",
    "        metric_single_row += f\"&${np.mean(single_average):.2f}\\\\pm{np.std(single_average):.2f}$\\\\\\\\\\n\"\n",
    "        metric_multi_row += f\"&${multi_average[0]:.2f}\\\\pm{multi_average[1]:.2f}$\\\\\\\\\\n\"\n",
    "        metric_rows.append(metric_single_row+metric_multi_row)\n",
    "    \n",
    "    output = dataset_template.substitute(\n",
    "        columns_c_string=column_c_string,\n",
    "        columns_table=columns_table,\n",
    "        variant_num=2,\n",
    "        f1_macro_scores=metric_rows[0],\n",
    "        f1_class0_scores=metric_rows[1],\n",
    "        f1_class1_scores=metric_rows[2],\n",
    "        dataset_name=dataset_names[dataset_idx],\n",
    "        dataset_label_name=dataset_label_names[dataset_idx],\n",
    "        experiment_name=experiment_names[-2]\n",
    "    )\n",
    "    return  output\n",
    "            \n",
    "            \n",
    "def generate_AB_table(dataset_idx):\n",
    "    experiment_column = \"Evaluation type\"\n",
    "    dataset = datasets[dataset_idx]\n",
    "    column_c_string = \"c\"*(len(LABEL_COLUMNS[dataset])+1)\n",
    "    columns_table = experiment_column + \"&\" + \" &\".join(LABEL_COLUMNS[dataset]) + \"& All Labels (average)\"\n",
    "    with open(join(datadir, filepath.format(dataset=dataset, exp=\"AA\")), \"r\") as f:\n",
    "        sup_data = json.load(f)\n",
    "    \n",
    "    with open(join(datadir, filepath.format(dataset=dataset, exp=\"BA\")), \"r\") as f:\n",
    "        selfsup_data = json.load(f)\n",
    "    \n",
    "    starting_rows = [\"&Supervised\", \"&Self-Supervised\"]\n",
    "    metric_rows = []\n",
    "    # for metric_single, metric_multi in zip((\"macro\", \"mean_for_0\", \"mean_for_1\"), metrics):\n",
    "    #     metric_single_row = \n",
    "    #     metric_multi_row = \n",
    "        # means, stds = get_mean_and_std(multi_data, metric_multi)\n",
    "        # multi_data_in_row = list(zip(list(means), list(stds)))\n",
    "        # multi_average = (means.mean(), means.std())\n",
    "        # single_average=[]\n",
    "#         for i, label in enumerate(single_data.keys()):\n",
    "#             curr_data = get_mean_and_std(single_data[label], metric_single)\n",
    "#             single_average.append(curr_data[0])\n",
    "#             metric_single_row += f\"&${curr_data[0]:.2f}\\\\pm{curr_data[1]:.2f}$\"\n",
    "#             curr_multi_data = multi_data_in_row[i]\n",
    "#             metric_multi_row += f\"&${curr_multi_data[0]:.2f}\\\\pm{curr_multi_data[1]:.2f}$\"\n",
    "            \n",
    "#             curr_row = \"& \" + str(key) + \" & \" + \" & \".join(map(lambda d: \"$\" + f\"{d[0]:.2f}\"+\"\\\\pm\"+f\"{d[1]:.2f}\" + \"$\", curr_data))\n",
    "#             curr_row += \"\\\\\\\\\\n\"\n",
    "#             metric_one_row += curr_row\n",
    "    for single_metric, metric in zip((\"macro\", \"mean_for_0\", \"mean_for_1\"), metrics):\n",
    "        metric_one_row = \"\"\n",
    "        for i, curr_json in enumerate((sup_data, selfsup_data)):\n",
    "            means, stds = get_mean_and_std(curr_json, metric)\n",
    "            curr_data = zip(list(means), list(stds))\n",
    "            mean, std = get_mean_and_std(curr_json, single_metric)\n",
    "\n",
    "            curr_row = starting_rows[i] + \" & \" + \" & \".join(map(lambda d: \"$\" + f\"{d[0]:.2f}\"+\"\\\\pm\"+f\"{d[1]:.2f}\" + \"$\", curr_data))\n",
    "            metric_one_row += curr_row + f\"&${mean:.2f}\"+\"\\\\pm\"+f\"{std:.2f}\" + \"$\"\n",
    "            metric_one_row += \"\\\\\\\\\\n\"\n",
    "        metric_rows.append(metric_one_row)\n",
    "            \n",
    "            \n",
    "        # metric_single_row += f\"&${np.mean(single_average):.2f}\\\\pm{np.std(single_average):.2f}$\\\\\\\\\\n\"\n",
    "        # metric_multi_row += f\"&${multi_average[0]:.2f}\\\\pm{multi_average[1]:.2f}$\\\\\\\\\\n\"\n",
    "        # metric_rows.append(metric_single_row+metric_multi_row)\n",
    "    \n",
    "    output = dataset_template.substitute(\n",
    "        columns_c_string=column_c_string,\n",
    "        columns_table=columns_table,\n",
    "        variant_num=2,\n",
    "        f1_macro_scores=metric_rows[0],\n",
    "        f1_class0_scores=metric_rows[1],\n",
    "        f1_class1_scores=metric_rows[2],\n",
    "        dataset_name=dataset_names[dataset_idx],\n",
    "        dataset_label_name=dataset_label_names[dataset_idx],\n",
    "        experiment_name=\"Self-Supervised\"\n",
    "    )\n",
    "    return  output\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "48f73f4d-2c50-4fd9-851e-948c3c047f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# out = generate_single_training_table(1)\n",
    "# out = generate_AB_table(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "painted-assistant",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# out = generate_single_table(0, -1) #, experiment_iterator=range(1,9), experiment_template=\"train_{v}\", experiment_column=\"Train folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52f0e74b-c086-4601-9580-6405893332fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_template = Template(\"\"\"\n",
    "\\\\begin{table}[H]\n",
    "\\\\centering\n",
    "\\\\begin{adjustbox}{width=\\\\textwidth}\n",
    "\\\\begin{tabular}{l|$columns_c_string}\n",
    "\\\\hline\n",
    " & \\\\multicolumn{$category_num}{*}{F1 macro} & \\\\multicolumn{$category_num}{*}{AER} & \\\\multicolumn{$category_num}{*}{AAL} & \\\\multicolumn{$category_num}{*}{MLRAL}\\\\\\\\\n",
    "\\\\hline\n",
    "Dataset & $category_variants & $category_variants & $category_variants & $category_variants\\\\\\\\\n",
    "\\\\hline\n",
    "Doccano 1.0  $doccano_scores\\\\\\\\\n",
    "\\\\hline\n",
    "MHS  $mhs_scores\\\\\\\\\n",
    "\\\\hline\n",
    "UC  $uc_scores\\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\end{adjustbox}\n",
    "\\\\caption{$experiment_name results for every dataset}\n",
    "\\\\label{tab:datasets_metrics_$experiment_label}\n",
    "\\\\end{table}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5f4a37ae-9211-4c82-a75c-fa123d06835c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = [\"macro\", \"AER\", \"AAL\", \"MLRAL\"]\n",
    "def generate_AB_sum_table():\n",
    "    category_variants = \"Supervised&Self-Supervised\"\n",
    "    category_num=2\n",
    "    columns_c_string = \"cc|cc|cc|cc\"\n",
    "    experiment_name = \"Self-Supervised\"\n",
    "    experiment_label= \"self_sup\"\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        with open(join(datadir, filepath.format(dataset=dataset, exp=\"AA\")), \"r\") as f:\n",
    "            sup_data = json.load(f)\n",
    "        with open(join(datadir, filepath.format(dataset=dataset, exp=\"BA\")), \"r\") as f:\n",
    "            selfsup_data = json.load(f)\n",
    "        row = \"\"\n",
    "        for metric in metrics:\n",
    "            mean, std = get_mean_and_std(sup_data, metric)\n",
    "            row += f\"&${mean:.2f}\"+\"\\\\pm\"+f\"{std:.2f}\" + \"$\"\n",
    "            mean, std = get_mean_and_std(selfsup_data, metric)\n",
    "            row += f\"&${mean:.2f}\"+\"\\\\pm\"+f\"{std:.2f}\" + \"$\"\n",
    "        row += \"\\\\\\\\\\n\"\n",
    "        if i == 0:\n",
    "            doccano_scores = row\n",
    "        elif i == 1:\n",
    "            mhs_scores = row\n",
    "        else:\n",
    "            uc_scores = row\n",
    "        \n",
    "    \n",
    "    out = results_template.substitute(\n",
    "        columns_c_string=columns_c_string,\n",
    "        category_num=category_num,\n",
    "        category_variants=category_variants,\n",
    "        doccano_scores=doccano_scores,\n",
    "        mhs_scores=mhs_scores,\n",
    "        uc_scores=uc_scores,\n",
    "        experiment_name=experiment_name,\n",
    "        experiment_label=experiment_label,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a3610d0e-f881-4d46-909a-3109b005c1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = generate_AB_sum_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "gothic-syndication",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(out_path, \"w\") as f:\n",
    "    f.write(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
